from langchain_chroma import Chroma
from langchain_ollama import OllamaLLM
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.runnables import RunnableLambda, RunnableMap
from langchain_core.prompts import PromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
# from huggingface_hub import login
#
#
# login("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")

# 1. ì„ë² ë”© ëª¨ë¸ ì„¤ì •
embedding_model = HuggingFaceEmbeddings(
    # model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
    model_name="princeton-nlp/sup-simcse-roberta-large",
    encode_kwargs={"normalize_embeddings": True}
)

# 2. ChromaDB ë²¡í„° ìŠ¤í† ì–´ ì—°ê²°
db = Chroma(
    persist_directory="./chroma",
    collection_name="my-docs",
    embedding_function=embedding_model,
)

# 3. ë¡œì»¬ LLM ì„¤ì • (Ollama ì‹¤í–‰ ì¤‘ì´ì–´ì•¼ í•¨)
llm = OllamaLLM(model="mistral")

# 4. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt = PromptTemplate.from_template("""
ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ê°œì¸ ë¬¸ì„œë§Œ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ëŠ” ë¹„ì„œì…ë‹ˆë‹¤.
ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”.
ë°˜ë“œì‹œ ì•„ë˜ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œë§Œ ë‹µë³€í•˜ê³ , ëª¨ë¥´ë©´ "ë¬¸ì„œì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µí•˜ì„¸ìš”.

ë¬¸ì„œ:
{context}

ì§ˆë¬¸:
{input}

ë‹µë³€:
""")


# 5. ë¬¸ì„œ ê²°í•© ì²´ì¸ êµ¬ì„± (stuff ë°©ì‹)
combine_docs_chain = create_stuff_documents_chain(llm, prompt)

# 6. RAG ì²´ì¸ êµ¬ì„±

retriever = db.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 5
    }
)

rag_chain = create_retrieval_chain(
    retriever,
    combine_docs_chain
)

# 7. ì‚¬ìš©ì ì…ë ¥ ë£¨í”„
if __name__ == "__main__":

    while True:
        query = input("ë¬´ì—‡ì´ ê¶ê¸ˆí•œê°€ìš”? (ì¢…ë£Œí•˜ë ¤ë©´ 'exit'): ")
        if query.lower() in ("exit", "quit"):
            break

        result = rag_chain.invoke({"input": query})

        print(f"\nğŸ§  ë‹µë³€: {result['answer']}\n")

        # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´ í™•ì¸
        for i, doc in enumerate(result["context"]):
            print(f"[{i+1}] {doc.page_content[:300]}")


